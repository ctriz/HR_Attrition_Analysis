{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b71b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextBlob\n",
      "  Obtaining dependency information for TextBlob from https://files.pythonhosted.org/packages/1e/d6/40aa5aead775582ea0cf35870e5a3f16fab4b967f1ad2debe675f673f923/textblob-0.19.0-py3-none-any.whl.metadata\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from TextBlob)\n",
      "  Obtaining dependency information for nltk>=3.9 from https://files.pythonhosted.org/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.9->TextBlob) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.9->TextBlob) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.9->TextBlob)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/ad/e0/8adc550d7169df1d6b9be8ff6019cda5291054a0107760c2f30788b6195f/regex-2025.9.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2025.9.1-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.5 kB 682.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 401.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.9->TextBlob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk>=3.9->TextBlob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 245.8/624.3 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 624.3/624.3 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 12.0 MB/s eta 0:00:00\n",
      "Downloading regex-2025.9.1-cp311-cp311-win_amd64.whl (276 kB)\n",
      "   ---------------------------------------- 0.0/276.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 276.2/276.2 kB 17.7 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, nltk, TextBlob\n",
      "Successfully installed TextBlob-0.19.0 nltk-3.9.1 regex-2025.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94eef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeNumber  Survey_Q1  Survey_Q2  Survey_Q3  Survey_Q4  Survey_Q5\n",
      "0               1          4          2          5          5          2\n",
      "1               2          5          2          1          1          1\n",
      "2               4          3          3          4          2          5\n",
      "3               5          5          5          2          4          1\n",
      "4               7          5          2          2          2          5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load IBM dataset CSV\n",
    "df_ibm = pd.read_csv('data\\HRAttrition.csv')\n",
    "\n",
    "# Assume 'EmployeeNumber' uniquely identifies each employee\n",
    "employee_ids = df_ibm['EmployeeNumber'].unique()\n",
    "\n",
    "# Settings for survey simulation\n",
    "np.random.seed(42)  # reproducibility\n",
    "num_employees = len(employee_ids)\n",
    "num_questions = 5  # number of survey questions\n",
    "\n",
    "# Generate synthetic survey data with scores 1 to 5 per question\n",
    "survey_data = {\n",
    "    'EmployeeNumber': employee_ids\n",
    "}\n",
    "for q in range(1, num_questions + 1):\n",
    "    survey_data[f'Survey_Q{q}'] = np.random.randint(1, 6, size=num_employees)\n",
    "\n",
    "# Create DataFrame of synthetic survey results\n",
    "df_survey = pd.DataFrame(survey_data)\n",
    "\n",
    "print(df_survey.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1ea19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
      "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
      "1   49        No  Travel_Frequently        279  Research & Development   \n",
      "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
      "3   33        No  Travel_Frequently       1392  Research & Development   \n",
      "4   27        No      Travel_Rarely        591  Research & Development   \n",
      "\n",
      "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
      "0                 1          2  Life Sciences              1               1   \n",
      "1                 8          1  Life Sciences              1               2   \n",
      "2                 2          2          Other              1               4   \n",
      "3                 3          4  Life Sciences              1               5   \n",
      "4                 2          1        Medical              1               7   \n",
      "\n",
      "   ...  WorkLifeBalance YearsAtCompany  YearsInCurrentRole  \\\n",
      "0  ...                1              6                   4   \n",
      "1  ...                3             10                   7   \n",
      "2  ...                3              0                   0   \n",
      "3  ...                3              8                   7   \n",
      "4  ...                3              2                   2   \n",
      "\n",
      "   YearsSinceLastPromotion  YearsWithCurrManager Survey_Q1  Survey_Q2  \\\n",
      "0                        0                     5         4          2   \n",
      "1                        1                     7         5          2   \n",
      "2                        0                     0         3          3   \n",
      "3                        3                     0         5          5   \n",
      "4                        2                     2         5          2   \n",
      "\n",
      "  Survey_Q3  Survey_Q4  Survey_Q5  \n",
      "0         5          5          2  \n",
      "1         1          1          1  \n",
      "2         4          2          5  \n",
      "3         2          4          1  \n",
      "4         2          2          5  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge dataframes on 'EmployeeNumber' (common key)\n",
    "merged_df = df_ibm.merge(df_survey, how='left', on='EmployeeNumber')\n",
    "\n",
    "# Preview merged dataframe\n",
    "print(merged_df.head())\n",
    "\n",
    "# Save merged dataframe to CSV\n",
    "#merged_df.to_csv('ibm_hr_with_survey.csv', index=False)\n",
    "\n",
    "#print(\"Merged dataset saved to 'ibm_hr_with_survey.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41df2d",
   "metadata": {},
   "source": [
    "Add free text engagemnt survey comments, do a sentiment analysis and score the engagement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030a0974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeNumber                           Survey_Text_Response  \\\n",
      "0               1         I frequently attend learning sessions.   \n",
      "1               2  I have clear goals for my career advancement.   \n",
      "2               4             I see limited progression options.   \n",
      "3               5            Training opportunities are limited.   \n",
      "4               7           Uncertain about my future path here.   \n",
      "\n",
      "   Sentiment_Polarity  Engagement_Score  \n",
      "0            0.100000                 3  \n",
      "1            0.100000                 3  \n",
      "2           -0.071429                 3  \n",
      "3           -0.071429                 3  \n",
      "4            0.000000                 3  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from textblob import TextBlob  # for simple sentiment analysis\n",
    "\n",
    "# Load IBM dataset CSV\n",
    "#df_ibm = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "#employee_ids = df_ibm['EmployeeNumber'].unique()\n",
    "\n",
    "# Sample phrases on different topics\n",
    "phrases = [\n",
    "    # Work-life balance\n",
    "    \"I balance my work and personal life well.\",\n",
    "    \"Long hours make it hard to spend time with family.\",\n",
    "    \"Flexible working hours help me stay motivated.\",\n",
    "    # Career plans\n",
    "    \"I have clear goals for my career advancement.\",\n",
    "    \"Uncertain about my future path here.\",\n",
    "    \"The company supports my growth plans.\",\n",
    "    # Learning\n",
    "    \"I frequently attend learning sessions.\",\n",
    "    \"Training opportunities are limited.\",\n",
    "    \"I enjoy the learning culture here.\",\n",
    "    # Career path\n",
    "    \"My career path aligns with my interests.\",\n",
    "    \"I see limited progression options.\",\n",
    "    \"I'm encouraged to explore new roles.\"\n",
    "]\n",
    "\n",
    "# Assign random phrase to each employee\n",
    "np.random.seed(42)\n",
    "assigned_phrases = np.random.choice(phrases, size=len(employee_ids))\n",
    "\n",
    "# Create DataFrame with EmployeeNumber and assigned phrase\n",
    "df_text = pd.DataFrame({\n",
    "    'EmployeeNumber': employee_ids,\n",
    "    'Survey_Text_Response': assigned_phrases\n",
    "})\n",
    "\n",
    "# Function to map polarity (-1 to +1) to score 1-5\n",
    "def polarity_to_score(polarity):\n",
    "    # Normalize polarity from [-1,1] to [1,5]\n",
    "    return int(round((polarity + 1) * 2 + 1))\n",
    "\n",
    "# Perform sentiment analysis on text and score engagement\n",
    "df_text['Sentiment_Polarity'] = df_text['Survey_Text_Response'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df_text['Engagement_Score'] = df_text['Sentiment_Polarity'].apply(polarity_to_score)\n",
    "\n",
    "# Show results\n",
    "print(df_text.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb05462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes on 'EmployeeNumber' (common key)\n",
    "merged_df = merged_df.merge(df_text, how='left', on='EmployeeNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8beb0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
      "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
      "1   49        No  Travel_Frequently        279  Research & Development   \n",
      "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
      "3   33        No  Travel_Frequently       1392  Research & Development   \n",
      "4   27        No      Travel_Rarely        591  Research & Development   \n",
      "\n",
      "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
      "0                 1          2  Life Sciences              1               1   \n",
      "1                 8          1  Life Sciences              1               2   \n",
      "2                 2          2          Other              1               4   \n",
      "3                 3          4  Life Sciences              1               5   \n",
      "4                 2          1        Medical              1               7   \n",
      "\n",
      "   ...  YearsSinceLastPromotion YearsWithCurrManager  Survey_Q1  Survey_Q2  \\\n",
      "0  ...                        0                    5          4          2   \n",
      "1  ...                        1                    7          5          2   \n",
      "2  ...                        0                    0          3          3   \n",
      "3  ...                        3                    0          5          5   \n",
      "4  ...                        2                    2          5          2   \n",
      "\n",
      "   Survey_Q3 Survey_Q4  Survey_Q5  \\\n",
      "0          5         5          2   \n",
      "1          1         1          1   \n",
      "2          4         2          5   \n",
      "3          2         4          1   \n",
      "4          2         2          5   \n",
      "\n",
      "                            Survey_Text_Response  Sentiment_Polarity  \\\n",
      "0         I frequently attend learning sessions.            0.100000   \n",
      "1  I have clear goals for my career advancement.            0.100000   \n",
      "2             I see limited progression options.           -0.071429   \n",
      "3            Training opportunities are limited.           -0.071429   \n",
      "4           Uncertain about my future path here.            0.000000   \n",
      "\n",
      "   Engagement_Score  \n",
      "0                 3  \n",
      "1                 3  \n",
      "2                 3  \n",
      "3                 3  \n",
      "4                 3  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8daa23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Join Years: [np.int64(1980), np.int64(1983), np.int64(1984), np.int64(1986), np.int64(1987), np.int64(1988), np.int64(1989), np.int64(1990), np.int64(1991), np.int64(1993), np.int64(1994), np.int64(1995), np.int64(1996), np.int64(1997), np.int64(1998), np.int64(1999), np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2003), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020)]\n",
      "Unique Leave Years: [np.int64(2020)]\n",
      "API arguments: from_year=1980, to_year=2020\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data\\HRAttrition.csv')\n",
    "\n",
    "# Inspect columns to identify or create joining and leaving years\n",
    "# For example, if 'DateOfJoining' exists, convert to datetime and extract year\n",
    "if 'DateOfJoining' in df.columns:\n",
    "    df['JoinYear'] = pd.to_datetime(df['DateOfJoining'], errors='coerce').dt.year\n",
    "else:\n",
    "    # If no date, try to infer from 'YearAtCompany' assuming dataset year known\n",
    "    # For example, assume dataset snapshot year 2020\n",
    "    snapshot_year = 2020\n",
    "    df['JoinYear'] = snapshot_year - df['YearsAtCompany']\n",
    "\n",
    "# For leaving year, assuming if Attrition='Yes' the leaving year is join year + YearsAtCompany\n",
    "df['LeaveYear'] = df['JoinYear'] + df['YearsAtCompany']\n",
    "\n",
    "# Get unique sorted years\n",
    "unique_join_years = sorted(df['JoinYear'].dropna().unique())\n",
    "unique_leave_years = sorted(df['LeaveYear'].dropna().unique())\n",
    "\n",
    "print(\"Unique Join Years:\", unique_join_years)\n",
    "print(\"Unique Leave Years:\", unique_leave_years)\n",
    "\n",
    "# For API arguments:\n",
    "from_year = min(unique_join_years)\n",
    "to_year = max(unique_leave_years)\n",
    "print(f\"API arguments: from_year={from_year}, to_year={to_year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188530dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unemployment Rate Data (1980-2020): Total 120 points\n",
      "Year 1989-December: 5.4%\n",
      "Year 1989-November: 5.4%\n",
      "Year 1989-October: 5.3%\n",
      "Year 1989-September: 5.3%\n",
      "Year 1989-August: 5.2%\n",
      "Year 1989-July: 5.2%\n",
      "Year 1989-June: 5.3%\n",
      "Year 1989-May: 5.2%\n",
      "Year 1989-April: 5.2%\n",
      "Year 1989-March: 5.0%\n",
      "Year 1989-February: 5.2%\n",
      "Year 1989-January: 5.4%\n",
      "Year 1988-December: 5.3%\n",
      "Year 1988-November: 5.3%\n",
      "Year 1988-October: 5.4%\n",
      "Year 1988-September: 5.4%\n",
      "Year 1988-August: 5.6%\n",
      "Year 1988-July: 5.4%\n",
      "Year 1988-June: 5.4%\n",
      "Year 1988-May: 5.6%\n",
      "Year 1988-April: 5.4%\n",
      "Year 1988-March: 5.7%\n",
      "Year 1988-February: 5.7%\n",
      "Year 1988-January: 5.7%\n",
      "Year 1987-December: 5.7%\n",
      "Year 1987-November: 5.8%\n",
      "Year 1987-October: 6.0%\n",
      "Year 1987-September: 5.9%\n",
      "Year 1987-August: 6.0%\n",
      "Year 1987-July: 6.1%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# BLS public API endpoint\n",
    "url = 'https://api.bls.gov/publicAPI/v2/timeseries/data/'\n",
    "\n",
    "# API parameters\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = json.dumps({\n",
    "    \"seriesid\": [\"LNS14000000\"],  # Unemployment rate national series\n",
    "    \"startyear\": \"1980\",\n",
    "    \"endyear\": \"2020\",\n",
    "    \"registrationKey\": \"\"  # Optional: Add your API key here if available\n",
    "})\n",
    "\n",
    "# Make POST request\n",
    "response = requests.post(url, data=data, headers=headers)\n",
    "\n",
    "# Parse JSON response\n",
    "json_data = response.json()\n",
    "\n",
    "# Print results summary\n",
    "if json_data['status'] == 'REQUEST_SUCCEEDED':\n",
    "    series_data = json_data['Results']['series'][0]['data']\n",
    "    print(f\"Unemployment Rate Data (1980-2020): Total {len(series_data)} points\")\n",
    "    for item in series_data[:30]:  # Show first 5 entries as sample\n",
    "        print(f\"Year {item['year']}-{item['periodName']}: {item['value']}%\")\n",
    "else:\n",
    "    print(\"API request failed:\", json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2279f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series data saved to 'bls_unemployment_series_data.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('bls_unemployment_series_data.json', 'w') as f:\n",
    "    json.dump(series_data, f, indent=2)\n",
    "\n",
    "print(\"Series data saved to 'bls_unemployment_series_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba177db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'year': '1989', 'period': 'M12', 'periodName': 'December', 'value': '5.4', 'footnotes': [{}]}, {'year': '1989', 'period': 'M11', 'periodName': 'November', 'value': '5.4', 'footnotes': [{}]}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('bls_unemployment_series_data.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "print(type(json_data))        # Should print <class 'list'> or <class 'dict'>\n",
    "print(json_data[:2])          # Print first 2 elements if it is a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b631fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON root type: <class 'list'>\n",
      "    year period periodName value footnotes\n",
      "0   1989    M12   December   5.4      [{}]\n",
      "1   1989    M11   November   5.4      [{}]\n",
      "2   1989    M10    October   5.3      [{}]\n",
      "3   1989    M09  September   5.3      [{}]\n",
      "4   1989    M08     August   5.2      [{}]\n",
      "5   1989    M07       July   5.2      [{}]\n",
      "6   1989    M06       June   5.3      [{}]\n",
      "7   1989    M05        May   5.2      [{}]\n",
      "8   1989    M04      April   5.2      [{}]\n",
      "9   1989    M03      March   5.0      [{}]\n",
      "10  1989    M02   February   5.2      [{}]\n",
      "11  1989    M01    January   5.4      [{}]\n",
      "12  1988    M12   December   5.3      [{}]\n",
      "13  1988    M11   November   5.3      [{}]\n",
      "14  1988    M10    October   5.4      [{}]\n",
      "15  1988    M09  September   5.4      [{}]\n",
      "16  1988    M08     August   5.6      [{}]\n",
      "17  1988    M07       July   5.4      [{}]\n",
      "18  1988    M06       June   5.4      [{}]\n",
      "19  1988    M05        May   5.6      [{}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'data.json' with your JSON filename\n",
    "with open('bls_unemployment_series_data.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Inspect top-level keys (headers)\n",
    "print(f\"JSON root type: {type(json_data)}\") \n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "bls_data = pd.DataFrame(json_data)\n",
    "\n",
    "# Display top 20 rows\n",
    "print(bls_data.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc68b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeNumber LeaveYear  UnemploymentRate       Trend\n",
      "0            1001      1988          5.491667  Decreasing\n",
      "1            1002      1989          5.258333  Decreasing\n",
      "2            1003      1987          6.175000  Decreasing\n",
      "3            1004      1990               NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load BLS JSON data from file\n",
    "with open('bls_unemployment_series_data.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# The root is a list of monthly records with keys: 'year', 'period', 'periodName', 'value', 'footnotes'\n",
    "# Convert to DataFrame\n",
    "df_bls = pd.DataFrame(json_data)\n",
    "\n",
    "# Convert 'value' column to float (unemployment rate)\n",
    "df_bls['value'] = df_bls['value'].astype(float)\n",
    "\n",
    "# Aggregate by year to get yearly average unemployment rate\n",
    "yearly_unemp = df_bls.groupby('year')['value'].mean().reset_index()\n",
    "yearly_unemp.rename(columns={'year': 'year', 'value': 'UnemploymentRate'}, inplace=True)\n",
    "\n",
    "# Sort by year for trend calculation\n",
    "yearly_unemp = yearly_unemp.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "# Create 'Trend' column - compare unemployment rate against previous year\n",
    "yearly_unemp['Trend'] = yearly_unemp['UnemploymentRate'].diff().apply(\n",
    "    lambda x: 'Increasing' if x > 0 else 'Decreasing'\n",
    ")\n",
    "# Fill NaN for first year\n",
    "yearly_unemp['Trend'] = yearly_unemp['Trend'].fillna('No Data')\n",
    "\n",
    "# Prepare employee DataFrame (example structure)\n",
    "# Assume df_emp has 'LeaveYear' column (int) representing the year employee left\n",
    "# For demonstration, creating a sample df_emp\n",
    "df_emp = pd.DataFrame({\n",
    "    'EmployeeNumber': [1001, 1002, 1003, 1004],\n",
    "    'LeaveYear': [1988, 1989, 1987, 1990]\n",
    "})\n",
    "\n",
    "# Convert 'LeaveYear' to string to match yearly_unemp 'year' dtype if needed\n",
    "df_emp['LeaveYear'] = df_emp['LeaveYear'].astype(str)\n",
    "\n",
    "# Merge employee data with unemployment data on LeaveYear/year\n",
    "merged_df = pd.merge(\n",
    "    df_emp,\n",
    "    yearly_unemp,\n",
    "    how='left',\n",
    "    left_on='LeaveYear',\n",
    "    right_on='year'\n",
    ")\n",
    "\n",
    "# Drop redundant 'year' column after merge\n",
    "merged_df.drop(columns=['year'], inplace=True)\n",
    "\n",
    "# Show final DataFrame with unemployment rate and trend corresponding to employee leaving year\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee64ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
