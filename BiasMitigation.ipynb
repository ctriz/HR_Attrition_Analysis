{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1355b6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairlearn in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fairlearn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=2.0.3 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fairlearn) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn>=1.2.1 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fairlearn) (1.7.1)\n",
      "Requirement already satisfied: scipy>=1.9.3 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fairlearn) (1.16.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.2.1->fairlearn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Obtaining dependency information for shap from https://files.pythonhosted.org/packages/aa/7c/eead607a358b2466b95b302e4a7eae717eeed53615653fbbd4c1527041b9/shap-0.48.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached shap-0.48.0-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (0.0.8)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Obtaining dependency information for numba>=0.54 from https://files.pythonhosted.org/packages/0f/a4/2b309a6a9f6d4d8cfba583401c7c2f9ff887adb5d54d8e2e130274c0973f/numba-0.61.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached numba-0.61.2-cp311-cp311-win_amd64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (4.15.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Obtaining dependency information for llvmlite<0.45,>=0.44.0dev0 from https://files.pythonhosted.org/packages/5f/c6/258801143975a6d09a373f2641237992496e15567b907a4d401839d671b8/llvmlite-0.44.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached llvmlite-0.44.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->shap) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ctrid\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Using cached shap-0.48.0-cp311-cp311-win_amd64.whl (544 kB)\n",
      "Using cached numba-0.61.2-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "Using cached llvmlite-0.44.0-cp311-cp311-win_amd64.whl (30.3 MB)\n",
      "Installing collected packages: llvmlite, numba, shap\n",
      "Successfully installed llvmlite-0.44.0 numba-0.61.2 shap-0.48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install fairlearn\n",
    "!pip install shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fe44d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, false_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your dataset (example)\n",
    "df = pd.read_csv('data\\HRAttrition-Revised.csv')\n",
    "\n",
    "# Features and target\n",
    "target = 'Attrition'  # binary target, e.g., 1 if attrition, 0 otherwise\n",
    "sensitive_cols = ['gender', 'religion', 'race', 'marital_status']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4664385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column labels in the dataset in column order:\n",
      "Age\n",
      "Attrition\n",
      "BusinessTravel\n",
      "DailyRate\n",
      "Department\n",
      "DistanceFromHome\n",
      "Education\n",
      "EducationField\n",
      "EnvironmentSatisfaction\n",
      "Gender\n",
      "HourlyRate\n",
      "JobInvolvement\n",
      "JobLevel\n",
      "JobRole\n",
      "JobSatisfaction\n",
      "MaritalStatus\n",
      "MonthlyIncome\n",
      "MonthlyRate\n",
      "NumCompaniesWorked\n",
      "OverTime\n",
      "PercentSalaryHike\n",
      "PerformanceRating\n",
      "RelationshipSatisfaction\n",
      "StockOptionLevel\n",
      "TotalWorkingYears\n",
      "TrainingTimesLastYear\n",
      "WorkLifeBalance\n",
      "YearsAtCompany\n",
      "YearsInCurrentRole\n",
      "YearsSinceLastPromotion\n",
      "YearsWithCurrManager\n",
      "gender\n",
      "religion\n",
      "race\n",
      "marital_status\n"
     ]
    }
   ],
   "source": [
    "print(\"Column labels in the dataset in column order:\")\n",
    "for column in df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85213773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         int64\n",
       "Attrition                   int64\n",
       "BusinessTravel              int64\n",
       "DailyRate                   int64\n",
       "Department                  int64\n",
       "DistanceFromHome            int64\n",
       "Education                   int64\n",
       "EducationField              int64\n",
       "EnvironmentSatisfaction     int64\n",
       "Gender                      int64\n",
       "HourlyRate                  int64\n",
       "JobInvolvement              int64\n",
       "JobLevel                    int64\n",
       "JobRole                     int64\n",
       "JobSatisfaction             int64\n",
       "MaritalStatus               int64\n",
       "MonthlyIncome               int64\n",
       "MonthlyRate                 int64\n",
       "NumCompaniesWorked          int64\n",
       "OverTime                    int64\n",
       "PercentSalaryHike           int64\n",
       "PerformanceRating           int64\n",
       "RelationshipSatisfaction    int64\n",
       "StockOptionLevel            int64\n",
       "TotalWorkingYears           int64\n",
       "TrainingTimesLastYear       int64\n",
       "WorkLifeBalance             int64\n",
       "YearsAtCompany              int64\n",
       "YearsInCurrentRole          int64\n",
       "YearsSinceLastPromotion     int64\n",
       "YearsWithCurrManager        int64\n",
       "gender                      int64\n",
       "religion                    int64\n",
       "race                        int64\n",
       "marital_status              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Encode categorical columns in df\n",
    "categorical_cols_df = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if categorical_cols_df:\n",
    "    for col in categorical_cols_df:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0fcf540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  gender      religion                race marital_status\n",
      "0   41  Female         Islam               White         Single\n",
      "1   49  Female  Christianity  Hispanic or Latino         Single\n",
      "2   37   Other       Judaism               Asian        Married\n",
      "3   33  Female         Islam  Hispanic or Latino       Divorced\n",
      "4   27  Female      Buddhism               Other       Divorced\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Age                          int64\n",
       "Attrition                    int64\n",
       "BusinessTravel               int64\n",
       "DailyRate                    int64\n",
       "Department                   int64\n",
       "DistanceFromHome             int64\n",
       "Education                    int64\n",
       "EducationField               int64\n",
       "EnvironmentSatisfaction      int64\n",
       "Gender                       int64\n",
       "HourlyRate                   int64\n",
       "JobInvolvement               int64\n",
       "JobLevel                     int64\n",
       "JobRole                      int64\n",
       "JobSatisfaction              int64\n",
       "MaritalStatus                int64\n",
       "MonthlyIncome                int64\n",
       "MonthlyRate                  int64\n",
       "NumCompaniesWorked           int64\n",
       "OverTime                     int64\n",
       "PercentSalaryHike            int64\n",
       "PerformanceRating            int64\n",
       "RelationshipSatisfaction     int64\n",
       "StockOptionLevel             int64\n",
       "TotalWorkingYears            int64\n",
       "TrainingTimesLastYear        int64\n",
       "WorkLifeBalance              int64\n",
       "YearsAtCompany               int64\n",
       "YearsInCurrentRole           int64\n",
       "YearsSinceLastPromotion      int64\n",
       "YearsWithCurrManager         int64\n",
       "gender                      object\n",
       "religion                    object\n",
       "race                        object\n",
       "marital_status              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Your existing DataFrame (load or assume named df)\n",
    "#df = pd.read_csv('ibm_hr_with_features.csv')  # example filename\n",
    "\n",
    "num_rows = df.shape[0]\n",
    "\n",
    "# Define plausible categories for each sensitive column\n",
    "genders = ['Male', 'Female', 'Other']\n",
    "religions = ['Christianity', 'Islam', 'Hinduism', 'Buddhism', 'Judaism', 'None', 'Other']\n",
    "races = ['White', 'Black or African American', 'Asian', 'Hispanic or Latino', 'Native American', 'Other']\n",
    "marital_statuses = ['Single', 'Married', 'Divorced', 'Widowed', 'Separated']\n",
    "\n",
    "# Generate random values for each column\n",
    "df['gender'] = np.random.choice(genders, size=num_rows, p=[0.48, 0.48, 0.04])\n",
    "df['religion'] = np.random.choice(religions, size=num_rows, p=[0.3, 0.25, 0.2, 0.1, 0.05, 0.05, 0.05])\n",
    "df['race'] = np.random.choice(races, size=num_rows, p=[0.5, 0.13, 0.18, 0.12, 0.03, 0.04])\n",
    "df['marital_status'] = np.random.choice(marital_statuses, size=num_rows, p=[0.4, 0.45, 0.1, 0.03, 0.02])\n",
    "\n",
    "# Verify the result\n",
    "\n",
    "print(df[['Age','gender', 'religion', 'race', 'marital_status']].head())\n",
    "df.dtypes\n",
    "\n",
    "# Save or continue processing as needed\n",
    "#df.to_csv('ibm_hr_with_sensitive_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8009bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode sensitive features for simplicity (alternative: label encoding or embedding)\n",
    "df_sensitive = pd.get_dummies(df[sensitive_cols])\n",
    "\n",
    "# For modeling, exclude sensitive features from X but keep them separately for fairness evaluation\n",
    "X = df.drop(columns=[target] + sensitive_cols)  # all features except sensitive and target\n",
    "y = df[target]\n",
    "\n",
    "# Keep sensitive features as a DataFrame for Fairlearn\n",
    "S = df[sensitive_cols]\n",
    "\n",
    "# Train/test split (stratify on target)\n",
    "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(\n",
    "    X, y, S, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5903b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition                                                 0  1\n",
      "gender religion race                      marital_status      \n",
      "Female Buddhism Asian                     Divorced        2  0\n",
      "                                          Married         5  1\n",
      "                                          Separated       1  0\n",
      "                                          Single          3  2\n",
      "                Black or African American Married         2  0\n",
      "...                                                      .. ..\n",
      "Other  None     Black or African American Single          1  0\n",
      "       Other    Black or African American Divorced        1  0\n",
      "                Hispanic or Latino        Married         1  0\n",
      "                                          Single          0  1\n",
      "                White                     Married         1  0\n",
      "\n",
      "[259 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "group_counts = pd.concat([y_train, S_train], axis=1).groupby(sensitive_cols)[target].value_counts().unstack(fill_value=0)\n",
    "print(group_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8acaa201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out degenerate sensitive groups (at least 2 classes in group)\n",
    "grouped = pd.concat([y_train, S_train], axis=1).groupby(sensitive_cols)\n",
    "valid_groups = grouped.filter(lambda x: x[target].nunique() > 1)\n",
    "\n",
    "X_train_filtered = X_train.loc[valid_groups.index]\n",
    "y_train_filtered = y_train.loc[valid_groups.index]\n",
    "S_train_filtered = S_train.loc[valid_groups.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d379a8",
   "metadata": {},
   "source": [
    "Approach 1: Using ExponentiatedGradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8e6b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.8435374149659864\n",
      "Baseline ROC AUC: 0.789042983891808\n"
     ]
    }
   ],
   "source": [
    "base_clf = RandomForestClassifier(random_state=42)\n",
    "base_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = base_clf.predict(X_test)\n",
    "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Baseline ROC AUC:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae8ed2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\_lagrangian.py:256: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas[h_idx] = h_gamma\n",
      "c:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\_lagrangian.py:257: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambdas[h_idx] = lambda_vec.copy()\n",
      "c:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\_lagrangian.py:256: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas[h_idx] = h_gamma\n",
      "c:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\_lagrangian.py:257: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambdas[h_idx] = lambda_vec.copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExponentiatedGradient Mitigator Results:\n",
      "Fairness-Adjusted Accuracy: 0.8469387755102041\n",
      "Approximate ROC AUC via unmitigated base classifier: 0.789042983891808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\exponentiated_gradient.py:346: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pred[t] = np.zeros(len(X))\n",
      "c:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\exponentiated_gradient.py:346: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pred[t] = np.zeros(len(X))\n"
     ]
    }
   ],
   "source": [
    "# Combine sensitive features into a single \"group\" series of tuples for Fairlearn\n",
    "# Wrap the base estimator with Fairlearn’s constraint\n",
    "mitigator = ExponentiatedGradient(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    constraints=DemographicParity()\n",
    ")\n",
    "\n",
    "mitigator.fit(X_train_filtered, y_train_filtered, sensitive_features=S_train_filtered)\n",
    "\n",
    "y_pred_fair = mitigator.predict(X_test)\n",
    "\n",
    "print(\"ExponentiatedGradient Mitigator Results:\")\n",
    "print(\"Fairness-Adjusted Accuracy:\", accuracy_score(y_test, y_pred_fair))\n",
    "\n",
    "# For ROC AUC, fallback to base classifier inside mitigator (approximate)\n",
    "\n",
    "# Use base_clf for predict_proba as approximation (no mitigation)\n",
    "y_proba_base = base_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Approximate ROC AUC via unmitigated base classifier:\", roc_auc_score(y_test, y_proba_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896e013",
   "metadata": {},
   "source": [
    "Apprroach 2: Using ThresholdOptimizer for Postprocessing (Supports predict_proba)\n",
    "ThresholdOptimizer is a postprocessing method that adjusts classification thresholds per group to satisfy fairness constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45ef6595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing Fairness Accuracy: 0.7891156462585034\n",
      "Base Classifier ROC AUC (no fairness adjustment): 0.7803428374536998\n"
     ]
    }
   ],
   "source": [
    "base_clf = RandomForestClassifier(random_state=42)\n",
    "base_clf.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "postprocessor = ThresholdOptimizer(\n",
    "    estimator=base_clf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method='predict_proba'\n",
    ")\n",
    "\n",
    "postprocessor.fit(X_train_filtered, y_train_filtered, sensitive_features=S_train_filtered)\n",
    "\n",
    "# Predict with fairness adjustments\n",
    "y_pred_post = postprocessor.predict(X_test, sensitive_features=S_test)\n",
    "\n",
    "print(\"Postprocessing Fairness Accuracy:\", accuracy_score(y_test, y_pred_post))\n",
    "\n",
    "# Use base classifier probabilities for approximate ROC AUC (no fairness adjustment)\n",
    "y_proba_base = base_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Base Classifier ROC AUC (no fairness adjustment):\", roc_auc_score(y_test, y_proba_base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f3249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness metrics by group:\n",
      "                                      accuracy  selection_rate  \\\n",
      "gender religion race  marital_status                             \n",
      "Female Buddhism Asian Divorced             NaN             NaN   \n",
      "                      Married              1.0             0.0   \n",
      "                      Separated            NaN             NaN   \n",
      "                      Single               0.0             0.0   \n",
      "                      Widowed              NaN             NaN   \n",
      "...                                        ...             ...   \n",
      "Other  Other    White Divorced             NaN             NaN   \n",
      "                      Married              NaN             NaN   \n",
      "                      Separated            NaN             NaN   \n",
      "                      Single               NaN             NaN   \n",
      "                      Widowed              NaN             NaN   \n",
      "\n",
      "                                      true_positive_rate  false_positive_rate  \n",
      "gender religion race  marital_status                                           \n",
      "Female Buddhism Asian Divorced                       NaN                  NaN  \n",
      "                      Married                        0.0                  0.0  \n",
      "                      Separated                      NaN                  NaN  \n",
      "                      Single                         0.0                  0.0  \n",
      "                      Widowed                        NaN                  NaN  \n",
      "...                                                  ...                  ...  \n",
      "Other  Other    White Divorced                       NaN                  NaN  \n",
      "                      Married                        NaN                  NaN  \n",
      "                      Separated                      NaN                  NaN  \n",
      "                      Single                         NaN                  NaN  \n",
      "                      Widowed                        NaN                  NaN  \n",
      "\n",
      "[630 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Use the sensitive features DataFrame directly\n",
    "group_test = S_test  # S_test is the sensitive features DataFrame used earlier\n",
    "\n",
    "metric_frame = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'selection_rate': selection_rate,\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'false_positive_rate': false_positive_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_fair,\n",
    "    sensitive_features=group_test\n",
    ")\n",
    "\n",
    "print(\"Fairness metrics by group:\")\n",
    "print(metric_frame.by_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0674ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e97a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness metrics by group:\n",
      "                                                  accuracy  selection_rate  \\\n",
      "sensitive_feature_0                                                          \n",
      "Female-Buddhism-Asian-Married                     1.000000             0.0   \n",
      "Female-Buddhism-Asian-Single                      0.000000             0.0   \n",
      "Female-Buddhism-Black or African American-Single  1.000000             0.0   \n",
      "Female-Buddhism-Hispanic or Latino-Single         1.000000             0.0   \n",
      "Female-Buddhism-White-Married                     0.888889             0.0   \n",
      "...                                                    ...             ...   \n",
      "Other-Buddhism-White-Divorced                     1.000000             0.0   \n",
      "Other-Christianity-White-Single                   0.500000             0.0   \n",
      "Other-Hinduism-White-Divorced                     1.000000             0.0   \n",
      "Other-Judaism-Asian-Married                       0.000000             0.0   \n",
      "Other-Judaism-Hispanic or Latino-Single           0.000000             0.0   \n",
      "\n",
      "                                                  true_positive_rate  \\\n",
      "sensitive_feature_0                                                    \n",
      "Female-Buddhism-Asian-Married                                    0.0   \n",
      "Female-Buddhism-Asian-Single                                     0.0   \n",
      "Female-Buddhism-Black or African American-Single                 0.0   \n",
      "Female-Buddhism-Hispanic or Latino-Single                        0.0   \n",
      "Female-Buddhism-White-Married                                    0.0   \n",
      "...                                                              ...   \n",
      "Other-Buddhism-White-Divorced                                    0.0   \n",
      "Other-Christianity-White-Single                                  0.0   \n",
      "Other-Hinduism-White-Divorced                                    0.0   \n",
      "Other-Judaism-Asian-Married                                      0.0   \n",
      "Other-Judaism-Hispanic or Latino-Single                          0.0   \n",
      "\n",
      "                                                  false_positive_rate  \n",
      "sensitive_feature_0                                                    \n",
      "Female-Buddhism-Asian-Married                                     0.0  \n",
      "Female-Buddhism-Asian-Single                                      0.0  \n",
      "Female-Buddhism-Black or African American-Single                  0.0  \n",
      "Female-Buddhism-Hispanic or Latino-Single                         0.0  \n",
      "Female-Buddhism-White-Married                                     0.0  \n",
      "...                                                               ...  \n",
      "Other-Buddhism-White-Divorced                                     0.0  \n",
      "Other-Christianity-White-Single                                   0.0  \n",
      "Other-Hinduism-White-Divorced                                     0.0  \n",
      "Other-Judaism-Asian-Married                                       0.0  \n",
      "Other-Judaism-Hispanic or Latino-Single                           0.0  \n",
      "\n",
      "[121 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combine multiple sensitive features into a single string group label\n",
    "\n",
    "group_test = S_test.astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "metric_frame = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'selection_rate': selection_rate,\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'false_positive_rate': false_positive_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_fair,\n",
    "    sensitive_features=group_test\n",
    ")\n",
    "\n",
    "print(\"Fairness metrics by group:\")\n",
    "print(metric_frame.by_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8bd07",
   "metadata": {},
   "source": [
    "Advance Data Prep -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8528cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df['ReciprocalYearsAtCompany'] = 1 / (df['YearsAtCompany'] + 1)\n",
    "df['RoleVsTenureRatio'] = df['YearsInCurrentRole'] / (df['YearsAtCompany'] + 1)\n",
    "\n",
    "df['TenureBucket'] = pd.cut(df['YearsAtCompany'], bins=[0,1,3,5,10,20], labels=False)\n",
    "df['PromotionRecencyBucket'] = pd.cut(df['YearsSinceLastPromotion'], bins=[-1,0,1,3,5,20], labels=False)\n",
    "\n",
    "df['RoleStabilityRatio'] = df['YearsInCurrentRole'] / (df['YearsAtCompany'] + 1)\n",
    "df['ManagerLoyaltyRatio'] = df['YearsWithCurrManager'] / (df['YearsAtCompany'] + 1)\n",
    "\n",
    "df['PromotionVelocity'] = df['YearsAtCompany'] / (df['YearsSinceLastPromotion'] + 1)\n",
    "df['RoleChangeVelocity'] = df['YearsAtCompany'] / (df['YearsInCurrentRole'] + 1)\n",
    "\n",
    "df['IsRecentPromotion'] = (df['YearsSinceLastPromotion'] <= 1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce82e78",
   "metadata": {},
   "source": [
    "Feature Selection using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32e714cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64      28\n",
      "bool       17\n",
      "float64     7\n",
      "Name: count, dtype: int64\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = df.drop(columns=['Attrition', 'YearsAtCompany', 'YearsSinceLastPromotion','YearsInCurrentRole','YearsWithCurrManager'])  # drop targets and dates\n",
    "target = df['Attrition']\n",
    "\n",
    "# Separate categorical and numeric columns\n",
    "cat_cols = features.select_dtypes(include=['object', 'category']).columns\n",
    "num_cols = features.select_dtypes(include=['number']).columns\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "features_encoded = pd.get_dummies(features, columns=list(cat_cols), drop_first=True)\n",
    "\n",
    "print(features_encoded.dtypes.value_counts())\n",
    "print(features_encoded.columns[features_encoded.dtypes == 'object'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f21a836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                                 0\n",
      "BusinessTravel                      0\n",
      "DailyRate                           0\n",
      "Department                          0\n",
      "DistanceFromHome                    0\n",
      "Education                           0\n",
      "EducationField                      0\n",
      "EnvironmentSatisfaction             0\n",
      "Gender                              0\n",
      "HourlyRate                          0\n",
      "JobInvolvement                      0\n",
      "JobLevel                            0\n",
      "JobRole                             0\n",
      "JobSatisfaction                     0\n",
      "MaritalStatus                       0\n",
      "MonthlyIncome                       0\n",
      "MonthlyRate                         0\n",
      "NumCompaniesWorked                  0\n",
      "OverTime                            0\n",
      "PercentSalaryHike                   0\n",
      "PerformanceRating                   0\n",
      "RelationshipSatisfaction            0\n",
      "StockOptionLevel                    0\n",
      "TotalWorkingYears                   0\n",
      "TrainingTimesLastYear               0\n",
      "WorkLifeBalance                     0\n",
      "ReciprocalYearsAtCompany            0\n",
      "RoleVsTenureRatio                   0\n",
      "TenureBucket                      110\n",
      "PromotionRecencyBucket              0\n",
      "RoleStabilityRatio                  0\n",
      "ManagerLoyaltyRatio                 0\n",
      "PromotionVelocity                   0\n",
      "RoleChangeVelocity                  0\n",
      "IsRecentPromotion                   0\n",
      "gender_Male                         0\n",
      "gender_Other                        0\n",
      "religion_Christianity               0\n",
      "religion_Hinduism                   0\n",
      "religion_Islam                      0\n",
      "religion_Judaism                    0\n",
      "religion_None                       0\n",
      "religion_Other                      0\n",
      "race_Black or African American      0\n",
      "race_Hispanic or Latino             0\n",
      "race_Native American                0\n",
      "race_Other                          0\n",
      "race_White                          0\n",
      "marital_status_Married              0\n",
      "marital_status_Separated            0\n",
      "marital_status_Single               0\n",
      "marital_status_Widowed              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(features_encoded.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17f76116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctrid\\AppData\\Local\\Temp\\ipykernel_17316\\3371284272.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_imputed[col].fillna(features_imputed[col].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "features_imputed = features_encoded.copy()\n",
    "\n",
    "# For numeric columns, fill NaNs with column mean\n",
    "for col in features_imputed.columns:\n",
    "    if features_imputed[col].dtype in ['float64', 'int64']:\n",
    "        features_imputed[col].fillna(features_imputed[col].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ecb72cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                               0\n",
      "BusinessTravel                    0\n",
      "DailyRate                         0\n",
      "Department                        0\n",
      "DistanceFromHome                  0\n",
      "Education                         0\n",
      "EducationField                    0\n",
      "EnvironmentSatisfaction           0\n",
      "Gender                            0\n",
      "HourlyRate                        0\n",
      "JobInvolvement                    0\n",
      "JobLevel                          0\n",
      "JobRole                           0\n",
      "JobSatisfaction                   0\n",
      "MaritalStatus                     0\n",
      "MonthlyIncome                     0\n",
      "MonthlyRate                       0\n",
      "NumCompaniesWorked                0\n",
      "OverTime                          0\n",
      "PercentSalaryHike                 0\n",
      "PerformanceRating                 0\n",
      "RelationshipSatisfaction          0\n",
      "StockOptionLevel                  0\n",
      "TotalWorkingYears                 0\n",
      "TrainingTimesLastYear             0\n",
      "WorkLifeBalance                   0\n",
      "ReciprocalYearsAtCompany          0\n",
      "RoleVsTenureRatio                 0\n",
      "TenureBucket                      0\n",
      "PromotionRecencyBucket            0\n",
      "RoleStabilityRatio                0\n",
      "ManagerLoyaltyRatio               0\n",
      "PromotionVelocity                 0\n",
      "RoleChangeVelocity                0\n",
      "IsRecentPromotion                 0\n",
      "gender_Male                       0\n",
      "gender_Other                      0\n",
      "religion_Christianity             0\n",
      "religion_Hinduism                 0\n",
      "religion_Islam                    0\n",
      "religion_Judaism                  0\n",
      "religion_None                     0\n",
      "religion_Other                    0\n",
      "race_Black or African American    0\n",
      "race_Hispanic or Latino           0\n",
      "race_Native American              0\n",
      "race_Other                        0\n",
      "race_White                        0\n",
      "marital_status_Married            0\n",
      "marital_status_Separated          0\n",
      "marital_status_Single             0\n",
      "marital_status_Widowed            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(features_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ca732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0f59c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_to_scale = features_imputed  # This should be your fully encoded DataFrame\n",
    "# Scale numeric columns only, or scale all (after encoding)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_to_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1358f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features via L1: ['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'ReciprocalYearsAtCompany', 'RoleVsTenureRatio', 'TenureBucket', 'PromotionRecencyBucket', 'RoleStabilityRatio', 'ManagerLoyaltyRatio', 'PromotionVelocity', 'RoleChangeVelocity', 'gender_Other', 'religion_Christianity', 'religion_Hinduism', 'religion_Islam', 'religion_Judaism', 'religion_Other', 'race_Black or African American', 'race_Hispanic or Latino', 'race_Native American', 'race_Other', 'race_White', 'marital_status_Married', 'marital_status_Separated', 'marital_status_Single', 'marital_status_Widowed']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# L1 logistic regression for feature importance\n",
    "lasso = LogisticRegression(penalty='l1', solver='saga', max_iter=5000, random_state=42)\n",
    "lasso.fit(X_scaled, target)\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "selected_features = features_imputed.columns[lasso.coef_[0] != 0]\n",
    "print(\"Selected features via L1:\", list(selected_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d839d0f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ctrid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import shap\n",
    "\n",
    "# Assume 'df' is your full DataFrame with target and features already prepared\n",
    "target = df['Attrition']\n",
    "\n",
    "# Use only the selected features from the L1 logistic regression step\n",
    "selected_features = ['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome',\n",
    "                     'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender',\n",
    "                     'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n",
    "                     'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime',\n",
    "                     'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel',\n",
    "                     'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
    "                     'ReciprocalYearsAtCompany', 'RoleVsTenureRatio', 'TenureBucket', 'PromotionRecencyBucket',\n",
    "                     'RoleStabilityRatio', 'ManagerLoyaltyRatio', 'PromotionVelocity', 'RoleChangeVelocity',\n",
    "                     'gender_Other', 'religion_Christianity', 'religion_Hinduism', 'religion_Islam',\n",
    "                     'religion_Judaism', 'religion_Other', 'race_Black or African American',\n",
    "                     'race_Hispanic or Latino', 'race_Native American', 'race_Other', 'race_White',\n",
    "                     'marital_status_Married', 'marital_status_Separated', 'marital_status_Single',\n",
    "                     'marital_status_Widowed']\n",
    "\n",
    "features_selected = df[selected_features]\n",
    "\n",
    "# Split data into train and test sets with stratification on target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_selected, target, test_size=0.2, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (important for many models and consistent feature scales)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest Classifier (robust and performs well on structured data)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]  # probabilities for ROC AUC\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Classification report on test set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Test ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "# Optional: cross-validation scores for more robust performance estimation\n",
    "cv_scores = cross_val_score(model, scaler.transform(features_selected), target, cv=5, scoring='roc_auc')\n",
    "print(f\"5-fold CV ROC AUC scores: {cv_scores}\")\n",
    "print(f\"Mean CV ROC AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# SHAP Explainability\n",
    "\n",
    "# Initialize SHAP TreeExplainer for Random Forest\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values on test set\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Global feature importance plot (summary)\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=selected_features)\n",
    "\n",
    "# Optional: Force plot for single prediction explanation (interactive)\n",
    "# shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_test.iloc[0,:], feature_names=selected_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
